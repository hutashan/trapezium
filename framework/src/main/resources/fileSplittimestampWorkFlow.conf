runMode = "BATCH"
dataSource = "HDFS"
singleRun=true
#HDFS File Batch configs
#For Batch transactions, details about the data is present here
#batchInfo - List of data sources for which RDD's will be created by the Application Manager
#batchInfo.name - Name of the batch datasource
#batchInfo.dataDirectory - HDFS directory where the datasource resides
#HDFS File Batch configs
#For Batch transactions, details about the data is present here
#batchInfo - List of data sources for which RDD's will be created by the Application Manager
#batchInfo.name - Name of the batch datasource
#batchInfo.dataDirectory - HDFS directory where the datasource resides
hdfsFileBatch = {
  batchTime = 5
  timerStartDelay = 1
  batchInfo = [
    {
      name = "testDataSplitFiles"
      dataDirectory = {
        local ="src/test/data/hdfs/filesplit/"
        query="*special*"
        }
      #split file into day. Processing file daywise.
      groupFile = {
        dateformat = "yyyy-MM-dd HH:mm"
        startDateIndex = 8
        endDateIndex = 24
      }
      validation = {
        columns = ["starttime","duration","subscriberid","destinationip","httpmethod","httphost","httpuri","httpuseragent","httpcontenttype","httpresponsecode","httpcontentlength","upstream_data","downstream_data","deviceid","network_type","ran_type","region_location","base_station_location","do_not_track","privacy"]
        datatypes = ["Long","Integer","String","String","String","String","String","String","String","String","String","String","String","String","String","String","String","String","String","String"]
        dateFormat = "yyyy-MM-dd HH:mm:ss"
        delimiter = ","
        minimumColumn = 20
        specialCharacterReplace = "\\000"
        rules = {
          subscriberid=[minLength(10),maxLength(20)]
          destinationip=[checkSpace]
        }
      }

    }
  ]
}
transactions = [{
  transactionName = "com.verizon.bda.trapezium.framework.apps.TestFileSplit"
  inputData = [{
    name = "testDataSplitFiles"
  }]
  persistDataName = "testOutput2"
}]
